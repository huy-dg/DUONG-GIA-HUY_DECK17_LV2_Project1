# Kafka â†’ MongoDB Streaming Pipeline

This project implements a **data streaming pipeline** that consumes real-time data from a Kafka topic and writes processed results into MongoDB.

It is built using:
- ğŸ **Python 3.13+**
- ğŸ§° **Poetry** for dependency management
- â˜ï¸ **Confluent Kafka** client
- ğŸƒ **MongoDB** for storage
- âš™ï¸ **Conda** for environment isolation



---

# PREREQUISITES

Miniconda or Anaconda MUST be install

Kafka containers MUST be started

MongoDB Community Edition MUST be install


---



# ğŸ“ Project structure
â”œâ”€â”€ kafka_project/ 

    â”œâ”€â”€ scripts/ 
    â”‚ â”œâ”€â”€ setup.py 
    â”‚ â”œâ”€â”€ streaming_data_from_source.py 
    â”‚ â””â”€â”€ consumer_to_mongodb.py 
    â”œâ”€â”€ src/ 
    â”‚ â””â”€â”€ kafka_project/ 
    â”‚ â”œâ”€â”€ __init__.py 
    â”‚ â”œâ”€â”€ core/ 
    â”‚ â”‚ â”œâ”€â”€ config.py 
    â”‚ â”‚ â”œâ”€â”€ logger.py 
    â”‚ â”‚ â””â”€â”€ json_processing.py 
    â”‚ â””â”€â”€ kafka/ 
    â”‚   â”œâ”€â”€ consumer.py 
    â”‚   â”œâ”€â”€ producer.py 
    â”‚   â””â”€â”€ topic.py 
    â”œâ”€â”€ tests/ 
    â”‚ â””â”€â”€ __init__.py 
    â”œâ”€â”€ .env 
    â”œâ”€â”€ environment.yml 
    â”œâ”€â”€ .gitignore 
    â”œâ”€â”€ poetry.lock 
    â”œâ”€â”€ pyproject.toml 
    â””â”€â”€ README.md

## âš™ï¸ Environment setup

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/huy-dg/DUONG-GIA-HUY_DECK17_LV2_Project1.git
mv DUONG-GIA-HUY_DECK17_LV2_Project1 "any_name"
cd "any_name"
```

### 2ï¸âƒ£ Create and activate Conda environment
```bash
conda env create -f environment.yml
conda activate test
```

### 3ï¸âƒ£ Install dependencies with Poetry
```bash
poetry install
```

### ğŸ”‘ Environment variables
Use the .env provided personally
or email me at duongghuy96@gmail.com

---



# ğŸš€ Running the pipeline
```bash
# Start the data source â†’ Kafka
python scripts/streaming_data_from_source.py

# Start the consumer â†’ MongoDB writer (in another terminal)
python scripts/consumer_to_mongodb.py
```

---

# ğŸ§± Development notes

All configuration is handled in src/kafka_project/core/config.py

JSON parsing and validation are handled in json_processing.py

Kafka topic, consumer, and producer utilities are under src/kafka_project/kafka/

Minor bugs: 

- consumer - mongoDB pipeline stopped after a while when waiting for message in topic.

- offset_metadata some how only got data of partition 0 and 2.

Improvement in next update:

- Will fix minor bugs.

- Will combine 2 script into 1 main.py script.

Future improvements:

- Rebuild source-to-producer pipeline with queuing model to improve throughput.

---

# ğŸ§‘â€ğŸ’» Author

huy-dg
ğŸ“§ duongghuy96@gmail.com
ğŸŒ https://github.com/huy-dg
